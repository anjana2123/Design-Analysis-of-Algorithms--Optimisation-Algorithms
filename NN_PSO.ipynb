{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cac759d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2580bf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs):\n",
    "        # Initialize the weights randomly\n",
    "        self.weights1 = np.random.randn(n_inputs, n_hidden)\n",
    "        self.weights2 = np.random.randn(n_hidden, n_outputs)\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        # Calculate the output of the neural network for input X\n",
    "        self.hidden = np.dot(X, self.weights1)\n",
    "        self.hidden_activated = self.sigmoid(self.hidden)\n",
    "        self.output = np.dot(self.hidden_activated, self.weights2)\n",
    "        self.output_activated = self.sigmoid(self.output)\n",
    "        return self.output_activated\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        # Sigmoid activation function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x - np.max(x))\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        hidden = self.sigmoid(np.dot(X, self.weights1))\n",
    "        output = self.softmax(np.dot(hidden, self.weights2))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d4291fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the particle swarm optimization class\n",
    "class ParticleSwarmOptimization:\n",
    "    def __init__(self, n_particles, n_inputs, n_hidden, n_outputs, max_iter, learning_rate):\n",
    "        # Initialize the swarm randomly\n",
    "        self.swarm = []\n",
    "        for i in range(n_particles):\n",
    "            neural_network = NeuralNetwork(n_inputs, n_hidden, n_outputs)\n",
    "            self.swarm.append(neural_network)\n",
    "\n",
    "        # Initialize the best position and error for each particle\n",
    "        self.best_positions = []\n",
    "        self.best_errors = []\n",
    "        for i in range(n_particles):\n",
    "            self.best_positions.append((self.swarm[i].weights1.copy(), self.swarm[i].weights2.copy()))\n",
    "            self.best_errors.append(float(\"inf\"))\n",
    "\n",
    "        # Initialize the global best position and error\n",
    "        self.global_best_position = None\n",
    "        self.global_best_error = float(\"inf\")\n",
    "\n",
    "        # Initialize the parameters for the PSO algorithm\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def calculate_error(self, X, y, neural_network):\n",
    "        y_pred = np.argmax(neural_network.forward(X), axis=1)\n",
    "        return np.mean(y_pred != y)\n",
    "\n",
    "    def optimize(self, X, y):\n",
    "        velocity1 = np.zeros_like(self.swarm[0].weights1)\n",
    "        velocity2 = np.zeros_like(self.swarm[0].weights2)\n",
    "        for i in range(self.max_iter):\n",
    "            for j in range(len(self.swarm)):\n",
    "                # Calculate the error for the current particle\n",
    "                error = self.calculate_error(X, y, self.swarm[j])\n",
    "\n",
    "                # Update the best position and error for the current particle\n",
    "                if error < self.best_errors[j]:\n",
    "                    self.best_positions[j] = (self.swarm[j].weights1.copy(), self.swarm[j].weights2.copy())\n",
    "                    self.best_errors[j] = error\n",
    "\n",
    "                # Update the global best position and error\n",
    "                if error < self.global_best_error:\n",
    "                    self.global_best_position = (self.swarm[j].weights1.copy(), self.swarm[j].weights2.copy())\n",
    "                    self.global_best_error = error\n",
    "\n",
    "                # Update the velocity and position for the current particle\n",
    "                r1 = np.random.rand(*self.swarm[j].weights1.shape)\n",
    "                r2 = np.random.rand(*self.swarm[j].weights2.shape)\n",
    "                velocity1 = self.learning_rate * (velocity1 + r1 * (self.best_positions[j][0] - self.swarm[j].weights1) + r2 * (self.global_best_position[0] - self.swarm[j].weights1))\n",
    "                velocity2 = self.learning_rate * (velocity2 + r1 * (self.best_positions[j][1] - self.swarm[j].weights2) + r2 * (self.global_best_position[1] - self.swarm[j].weights2))\n",
    "                self.swarm[j].weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3febb5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r'C:\\Users\\anjan\\Downloads\\P16-Artificial-Neural-Networks\\Part 1 - Artificial Neural Networks\\Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9170f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 3:-1].values\n",
    "y = data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36391baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X[:, 2] = le.fit_transform(X[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31b9bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6c4ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f522421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3083367",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_particles = 10\n",
    "n_inputs = X_train.shape[1]\n",
    "n_hidden = 5\n",
    "n_outputs = len(np.unique(y_train))\n",
    "max_iter = 50\n",
    "learning_rate = 0.5\n",
    "pso = ParticleSwarmOptimization(n_particles, n_inputs, n_hidden, n_outputs, max_iter, learning_rate)\n",
    "\n",
    "# Train the neural network using PSO\n",
    "pso.optimize(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the neural network on the testing set\n",
    "y_pred = np.argmax(pso.global_best_position[1] @ pso.global_best_position[0].T @ X_test.T, axis=0)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
